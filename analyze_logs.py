#!/usr/bin/env python3
"""
analyze_logs.py

Analyzes the session_data.csv generated by the PilotNet inference script (main_PilotNetJunction.py).
It computes various performance metrics, generates plots (including heatmaps
and time-lag analyses), and writes a report (with saved figures) to the same
directory as the CSV file.

In particular, this script adds:
  1) A "heatmap" of steering angle classes vs RoadOptions aka Navigation Commands
  2) A properly-labeled bar chart for the distribution of steering angle classes
     (based on boundaries = [-1.0, -0.8, -0.6, -0.4, -0.2, -0.1, -0.05, -0.02,
                             0.0, 0.02, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0])
  3) Some extra metrics and plots, including time-lag correlation
     between confidence and future cross-track error (to see if low confidence
     precedes higher CTE). This is more experimental...
"""


import os
import sys

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # For headless environments
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.lines import Line2D

from PilotNetJunctionBase import BasePilotNet
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix




# Steering Boundaries
# helper class
pilotnet = BasePilotNet({}, create_directories=False)
STEERING_BOUNDARIES = pilotnet.boundaries
HIGH_CTE_THRESH = 0.5


def steering_class_to_label(class_idx, boundaries=STEERING_BOUNDARIES):
    """
    Returns a string label for the class index based on the boundaries.
    Example: class_idx=0 -> '-1.0', class_idx=16 -> '1.0'
    """
    if 0 <= class_idx < len(boundaries):
        return f"{boundaries[class_idx]:.2f}"
    else:
        return "Unknown"

# Entropy Calculation
def compute_entropy(probabilities):
    probabilities = probabilities[probabilities > 0]  # Avoid log(0)
    return -np.sum(probabilities * np.log(probabilities))



# Data Loading and Preprocessing
def load_and_preprocess_data(csv_path):
    """
    Loads session_data.csv into a pandas DataFrame.
    - Interpolates missing IMU data (should be complete tho)
    - Computes traveled distance (Meters)
    - Computes velocity magnitude
    - Adds 'PilotNetSteeringClass' if 'PilotNetSteeringAngle' exists
    - Adds 'MaxPredictionConfidence' if Probability_Class_* columns exist
    - Creates 'IsIntervention' if 'ExecutedBy' not PilotNet
    - Also creates a time_delta for advanced time-based metrics
    """
    data = pd.read_csv(csv_path)

    # Replace zero IMU values with NaN, then interpolate
    imu_data_cols = ['AccelX', 'AccelY', 'AccelZ', 'AngularVelX', 'AngularVelY', 'AngularVelZ']
    for col in imu_data_cols:
        if col in data.columns:
            data[col] = data[col].replace(0, np.nan)
    data[imu_data_cols] = data[imu_data_cols].interpolate(method='linear')

    # Compute traveled distance
    data['X_Shift'] = data['X'].shift()
    data['Y_Shift'] = data['Y'].shift()
    data['Distance'] = np.sqrt((data['X'] - data['X_Shift'])**2 + (data['Y'] - data['Y_Shift'])**2)
    data['Meters'] = data['Distance'].cumsum()
    data.drop(['X_Shift', 'Y_Shift', 'Distance'], axis=1, inplace=True)

    # Compute velocity magnitude
    data['Velocity'] = np.sqrt(data['VelX']**2 + data['VelY']**2 + data['VelZ']**2)

    # Create a time_delta (in seconds) for each row
    data['time_delta'] = data['Timestamp'].diff().fillna(0.0)

    # Max confidence if Probability columns exist
    confidence_columns = [col for col in data.columns if col.startswith("Probability_Class_")]
    if confidence_columns:
        data['MaxPredictionConfidence'] = data[confidence_columns].max(axis=1)
    else:
        data['MaxPredictionConfidence'] = np.nan  # no probabilities available

    # Create a helper column to identify "Interventions"
    if 'ExecutedBy' in data.columns:
        data['IsIntervention'] = data['ExecutedBy'].isin(['Stanley', 'Intervention(Manual)', 'Intervention(LowSpeed)'])
    else:
        data['IsIntervention'] = False

    # Create "PilotNetSteeringClass"
    if 'PilotNetSteeringAngle' in data.columns:
        data['PilotNetSteeringClass'] = data['PilotNetSteeringAngle'].apply(
            lambda angle: pilotnet.map_label_to_class(angle)
        )

    # Create "Entropy" column
    probability_columns = [col for col in data.columns if col.startswith("Probability_Class_")]
    data['Entropy'] = data[probability_columns].apply(compute_entropy, axis=1)

    return data


def compute_ordinal_accuracy(y_true, y_pred, tolerance=1):
    """
    Computes accuracy allowing for predictions within a certain distance of true class.
    Args:
        y_true: True class indices
        y_pred: Predicted class indices
        tolerance: Number of classes to tolerate in either direction
    Returns:
        float: Accuracy score considering ordinal nature
    """
    correct = np.abs(y_true - y_pred) <= tolerance
    return np.mean(correct)


def compute_mean_absolute_error_classes(y_true, y_pred):
    """
    Computes mean absolute error in terms of class indices.
    This reflects how far off predictions are in terms of class distance.
    """
    return np.mean(np.abs(y_true - y_pred))


def compute_steering_classification_metrics(data):
    """
    Computes classification metrics for steering angle prediction:
    - Overall accuracy (exact and with tolerance)
    - Mean absolute error in class indices
    - Per-class precision, recall, F1
    - Confusion matrix
    - Accuracy per road option

    Args:
        data (pd.DataFrame): DataFrame with StanleySteeringAngle and PilotNetSteeringAngle

    Returns:
        dict: Dictionary containing all metrics and the confusion matrix
    """

    def map_label_to_class(label):
        differences = np.abs(label - STEERING_BOUNDARIES)
        return np.argmin(differences)

    # Convert angles to classes
    y_true = data['StanleySteeringAngle'].apply(map_label_to_class)
    y_pred = data['PilotNetSteeringAngle'].apply(map_label_to_class)

    # Compute basic metrics
    exact_accuracy = accuracy_score(y_true, y_pred)

    # Compute ordinal metrics
    tolerance_1_accuracy = compute_ordinal_accuracy(y_true, y_pred, tolerance=1)
    tolerance_2_accuracy = compute_ordinal_accuracy(y_true, y_pred, tolerance=2)
    mae_classes = compute_mean_absolute_error_classes(y_true, y_pred)

    # Compute per-class metrics using all possible classes
    all_classes = np.arange(len(STEERING_BOUNDARIES))
    precision, recall, f1, support = precision_recall_fscore_support(
        y_true, y_pred,
        labels=all_classes,
        zero_division=0
    )

    # Compute confusion matrix for all possible classes
    conf_matrix = confusion_matrix(
        y_true, y_pred,
        labels=all_classes
    )

    # Compute accuracy per road option if available
    accuracy_by_road_option = {}
    tolerance_1_accuracy_by_road_option = {}
    if 'CurrentRoadOption' in data.columns:
        for road_option in data['CurrentRoadOption'].unique():
            mask = data['CurrentRoadOption'] == road_option
            if mask.any():
                acc = accuracy_score(y_true[mask], y_pred[mask])
                accuracy_by_road_option[road_option] = acc

                # Also compute ordinal accuracy for each road option
                ord_acc = compute_ordinal_accuracy(y_true[mask], y_pred[mask], tolerance=1)
                tolerance_1_accuracy_by_road_option[road_option] = ord_acc

    # Create class labels using the provided function
    class_labels = [steering_class_to_label(i, STEERING_BOUNDARIES) for i in range(len(STEERING_BOUNDARIES))]

    # Organize results
    metrics = {
        'exact_accuracy': exact_accuracy,
        'tolerance_1_accuracy': tolerance_1_accuracy,
        'tolerance_2_accuracy': tolerance_2_accuracy,
        'mean_absolute_error_classes': mae_classes,
        'per_class_metrics': {
            'class_labels': class_labels,
            'precision': precision.tolist(),
            'recall': recall.tolist(),
            'f1': f1.tolist(),
            'support': support.tolist()
        },
        'confusion_matrix': conf_matrix,
        'confusion_matrix_labels': class_labels,
        'accuracy_by_road_option': accuracy_by_road_option,
        'tolerance_1_accuracy_by_road_option': tolerance_1_accuracy_by_road_option
    }

    return metrics


def plot_classification_metrics(metrics, output_dir):
    """
    Creates and saves visualization plots for the classification metrics.

    Args:
        metrics (dict): Output from compute_steering_classification_metrics
        output_dir (str): Directory to save the plots
    """
    # 1. Plot confusion matrix
    plt.figure(figsize=(15, 12))
    sns.heatmap(
        metrics['confusion_matrix'],
        annot=True,
        fmt='d',
        cmap='YlOrRd',
        xticklabels=metrics['confusion_matrix_labels'],
        yticklabels=metrics['confusion_matrix_labels']
    )
    plt.title('Steering Angle Classification Confusion Matrix')
    plt.xlabel('Predicted Steering Angle')
    plt.ylabel('True Steering Angle')
    plt.xticks(rotation=45)
    plt.yticks(rotation=45)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'steering_confusion_matrix.png'))
    plt.close()

    # 2. Plot per-class metrics
    class_labels = metrics['per_class_metrics']['class_labels']
    precision = np.array(metrics['per_class_metrics']['precision'])
    recall = np.array(metrics['per_class_metrics']['recall'])
    f1 = np.array(metrics['per_class_metrics']['f1'])
    support = np.array(metrics['per_class_metrics']['support'])

    # Only plot classes that have any samples
    valid_mask = support > 0
    class_labels = [label for i, label in enumerate(class_labels) if valid_mask[i]]
    precision = precision[valid_mask]
    recall = recall[valid_mask]
    f1 = f1[valid_mask]

    plt.figure(figsize=(15, 6))
    x = np.arange(len(class_labels))
    width = 0.25

    plt.bar(x - width, precision, width, label='Precision')
    plt.bar(x, recall, width, label='Recall')
    plt.bar(x + width, f1, width, label='F1')

    plt.xlabel('Steering Angle')
    plt.ylabel('Score')
    plt.title('Per-Class Classification Metrics (Classes with Support > 0)')
    plt.xticks(x, class_labels, rotation=45)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'per_class_metrics.png'))
    plt.close()

    # 3. Plot accuracy by road option if available
    if metrics['accuracy_by_road_option']:
        plt.figure(figsize=(12, 6))
        road_options = list(metrics['accuracy_by_road_option'].keys())
        exact_accuracies = list(metrics['accuracy_by_road_option'].values())
        ordinal_accuracies = list(metrics['tolerance_1_accuracy_by_road_option'].values())

        x = np.arange(len(road_options))
        width = 0.35

        plt.bar(x - width / 2, exact_accuracies, width, label='Exact Accuracy')
        plt.bar(x + width / 2, ordinal_accuracies, width, label='Ordinal Accuracy (±1 class)')

        plt.axhline(y=metrics['exact_accuracy'], color='r', linestyle='--',
                    label=f'Overall Exact Accuracy: {metrics["exact_accuracy"]:.3f}')
        plt.axhline(y=metrics['tolerance_1_accuracy'], color='g', linestyle='--',
                    label=f'Overall Ordinal Accuracy: {metrics["tolerance_1_accuracy"]:.3f}')

        plt.xlabel('Road Option')
        plt.ylabel('Accuracy')
        plt.title('Steering Prediction Accuracy by Road Option')
        plt.xticks(x, road_options, rotation=45)
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'accuracy_by_road_option.png'))
        plt.close()



# Metric Computations
def compute_general_metrics(data):
    """
    Basic stats with more detailed CTE analysis
    """
    if len(data) > 0:
        total_distance_km = round(data['Meters'].iloc[-1] / 1000.0, 4)
    else:
        total_distance_km = 0.0

    avg_velocity = round(data['Velocity'].mean(), 4) if 'Velocity' in data.columns else np.nan

    # Detailed CTE Analysis
    if 'CrossTrackError' in data.columns:
        # Overall CTE
        min_cte = round(data['CrossTrackError'].min(), 4)
        max_cte = round(data['CrossTrackError'].max(), 4)
        mean_cte = round(data['CrossTrackError'].mean(), 4)
        abs_mean_cte = round(data['CrossTrackError'].abs().mean(), 4)

        # Lane Following CTE
        lanefollow_mask = data['CurrentRoadOption'] == 'LANEFOLLOW'
        lanefollow_abs_mean_cte = round(data.loc[lanefollow_mask, 'CrossTrackError'].abs().mean(), 4)

        # Junction CTE
        junction_mask = data['IsCarAtJunction'] == True
        junction_abs_mean_cte = round(data.loc[junction_mask, 'CrossTrackError'].abs().mean(), 4)
    else:
        min_cte = max_cte = mean_cte = abs_mean_cte = lanefollow_abs_mean_cte = junction_abs_mean_cte = np.nan

    # RoadOption distribution
    if 'CurrentRoadOption' in data.columns:
        road_option_counts = data['CurrentRoadOption'].value_counts().to_dict()
    else:
        road_option_counts = {}

    # Steering mode distribution
    if 'ExecutedBy' in data.columns:
        steering_mode_counts = data['ExecutedBy'].value_counts().to_dict()
    else:
        steering_mode_counts = {}

    general_metrics = {
        "Total Distance (km)": total_distance_km,
        "Average Velocity (m/s)": avg_velocity,
        "Min CTE (m)": min_cte,
        "Max CTE (m)": max_cte,
        "Mean CTE (m)": mean_cte,
        "Overall Abs Mean CTE (m)": abs_mean_cte,
        "LaneFollow Abs Mean CTE (m)": lanefollow_abs_mean_cte,
        "Junction Abs Mean CTE (m)": junction_abs_mean_cte,
        "RoadOptionDistribution": road_option_counts,
        "SteeringModeDistribution": steering_mode_counts
    }
    return general_metrics


def analyze_entropy(data):
    """
    Analyze how entropy evolves over time, behaves at junctions, and correlates with high CTE.
    """
    # Entropy over time
    entropy_over_time = data[['Timestamp', 'Entropy']].copy()

    # Entropy at junctions
    junction_entropy = data[data['IsCarAtJunction']]['Entropy'].describe().to_dict()

    # Entropy correlation with high CTE
    high_cte_threshold = HIGH_CTE_THRESH
    high_cte_entropy = data[data['CrossTrackError'].abs() > high_cte_threshold]['Entropy'].describe().to_dict()

    entropy_analysis = {
        "OverallEntropyStats": data['Entropy'].describe().to_dict(),
        "EntropyOverTime": entropy_over_time,
        "JunctionEntropy": junction_entropy,
        "HighCTEEntropy": high_cte_entropy,
    }
    return entropy_analysis

def compute_intervention_metrics(data):
    """
    Detailed intervention analysis including junction-specific counts
    """
    if len(data) < 2:
        return {
            "InterventionsCount": 0,
            "JunctionInterventionsCount": 0,
            "InterventionsPerKm": 0.0,
            "InterventionsPerHour": 0.0,
            "TimeInInterventionPerHour": 0.0,
            "LongestPilotNetDistance(m)": 0.0,
            "LongestPilotNetTime(s)": 0.0
        }

    distance_km = round(data['Meters'].iloc[-1] / 1000.0, 4)
    total_time_hours = round((data['Timestamp'].iloc[-1] - data['Timestamp'].iloc[0]) / 3600.0, 4)

    # Count transitions from PilotNet to intervention
    data['PrevExecutedBy'] = data['ExecutedBy'].shift(1)
    transitions = data[
        (data['PrevExecutedBy'] == 'PilotNet') &
        (data['ExecutedBy'].isin(['Stanley','Intervention(Manual)','Intervention(LowSpeed)']))
    ]
    interventions_count = len(transitions)

    # Count junction-specific interventions
    junction_transitions = transitions[transitions['IsCarAtJunction'] == True]
    junction_interventions_count = len(junction_transitions)

    if distance_km > 0.0:
        interventions_per_km = round(interventions_count / distance_km, 4)
    else:
        interventions_per_km = 0.0

    if total_time_hours > 0.0:
        interventions_per_hour = round(interventions_count / total_time_hours, 4)
    else:
        interventions_per_hour = 0.0

    # Time in intervention
    time_in_intervention_seconds = data.loc[data['IsIntervention'] == True, 'time_delta'].sum()
    time_in_intervention_hours = time_in_intervention_seconds / 3600.0
    time_in_intervention_percentage = round(time_in_intervention_hours / total_time_hours * 100, 4)

    # Find the longest continuous PilotNet segment
    data['PilotNetSegmentID'] = ((data['ExecutedBy'] != 'PilotNet')
                                 | (data['ExecutedBy'].shift(1) != 'PilotNet')).cumsum()
    pilotnet_rows = data[data['ExecutedBy'] == 'PilotNet'].copy()
    pilotnet_rows['SegmentDelta'] = pilotnet_rows['Meters'].diff().fillna(0.0)
    pilotnet_rows['TimeDelta'] = pilotnet_rows['Timestamp'].diff().fillna(0.0)

    segment_stats = pilotnet_rows.groupby('PilotNetSegmentID').agg({
        'SegmentDelta': 'sum',
        'TimeDelta': 'sum'
    })
    longest_pilotnet_distance = round(segment_stats['SegmentDelta'].max() if not segment_stats.empty else 0.0, 4)
    longest_pilotnet_time = round(segment_stats['TimeDelta'].max() if not segment_stats.empty else 0.0, 4)

    metrics = {
        "InterventionsCount": interventions_count,
        "JunctionInterventionsCount": junction_interventions_count,
        "InterventionsPerKm": interventions_per_km,
        "InterventionsPerHour": interventions_per_hour,
        "TimeInInterventionPercentage": time_in_intervention_percentage,
        "LongestPilotNetDistance(m)": longest_pilotnet_distance,
        "LongestPilotNetTime(s)": longest_pilotnet_time
    }
    return metrics

def analyze_failures_and_confidence(data):
    """
    Analyzes failure scenarios and correlates them with pilotnet confidence.
    - Failure = ExecutedBy != 'PilotNet'
    - Confidence thresholds
    - Transitions to Non-Pilot
    Returns a dict with aggregated statistics.
    """
    if 'ExecutedBy' in data.columns:
        data['IsFailure'] = data['ExecutedBy'] != 'PilotNet'
    else:
        data['IsFailure'] = False

    # Confidence analysis
    if 'MaxPredictionConfidence' in data.columns and data['MaxPredictionConfidence'].notnull().any():
        thresholds = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]
        cte_by_confidence = {}
        for th in thresholds:
            mask = data['MaxPredictionConfidence'] >= th
            cte_by_confidence[f"Mean Abs CTE (conf >= {th})"] = data.loc[mask, 'CrossTrackError'].abs().mean()
        cte_by_confidence["Global Mean Abs CTE"] = data['CrossTrackError'].abs().mean()
    else:
        cte_by_confidence = {"No Probability Columns Found": None}

    # Summaries
    failure_count = data['IsFailure'].sum()
    total_count = len(data)

    data['PrevExecutedBy'] = data['ExecutedBy'].shift(1)
    transitions = data[(data['ExecutedBy'] != 'PilotNet') & (data['PrevExecutedBy'] == 'PilotNet')]
    transitions_count = len(transitions)

    stats = {
        "FailureCount": failure_count,
        "TotalCount": total_count,
        "Transitions to Non-PilotNet": transitions_count,
        "ConfidenceStats": cte_by_confidence
    }
    return stats


def compute_time_lag_confidence_cte(data, max_lag_seconds=5.0, step=0.1):
    """
    Examines how the current confidence correlates with cross-track error
    in the future. The hypothesis: Lower confidence at time t leads to higher
    CTE at time t+delta.

    We will sample time shifts: [1.0, 2.0, 3.0, ... up to max_lag_seconds]
    For each row at time t, find the nearest row in the future at t + shift,
    and compute correlation between confidence(t) and cte(t+shift).

    Returns a dict: shift -> correlation_coefficient
    """
    # If no confidence column, return empty
    if 'MaxPredictionConfidence' not in data.columns or data['MaxPredictionConfidence'].isnull().all():
        return {}

    # A basic approach using a merged DataFrame (self-join on nearest time).
    results = {}
    # Make a copy with a rename to align
    df_copy = data[['Timestamp','CrossTrackError']].copy()
    df_copy.rename(columns={'Timestamp': 'Timestamp_future',
                            'CrossTrackError': 'CTE_future'}, inplace=True)

    # Iterate over possible lags
    time_shifts = np.arange(0, max_lag_seconds + step, step)
    for lag in time_shifts:
        # For each row in `data`, we want to find the row in `df_copy` whose
        # Timestamp_future is closest to Timestamp + lag
        merged = data[['Timestamp','MaxPredictionConfidence']].copy()
        merged['TargetTime'] = merged['Timestamp'] + lag

        # Do an asof merge on Timestamp_future
        # Sort both dataframes by ascending Timestamp
        merged.sort_values('TargetTime', inplace=True)
        df_copy.sort_values('Timestamp_future', inplace=True)

        # Use pandas merge_asof: left_on='TargetTime', right_on='Timestamp_future'
        # This will find the nearest row in df_copy for each row in merged,
        # but only with a timestamp_future >= target_time by default.
        merged2 = pd.merge_asof(
            merged,
            df_copy,
            left_on='TargetTime',
            right_on='Timestamp_future',
            direction='forward'
        )

        # Now merged2 has: Timestamp, MaxPredictionConfidence, TargetTime, CTE_future
        # Compute correlation (pearson) between confidence and CTE_future
        valid_rows = merged2.dropna(subset=['MaxPredictionConfidence','CTE_future'])
        if len(valid_rows) > 2:
            corr_val = valid_rows['MaxPredictionConfidence'].corr(valid_rows['CTE_future'].abs())
        else:
            corr_val = np.nan

        results[f"{lag:.1f}"] = corr_val

    return results


# Plotting
def generate_plots(data, output_dir):
    """
    Creates and saves relevant plots to the output directory, including:
      - RoadOptionDistribution bar plot
      - Steering Angle class distribution (with boundary-based labels)
      - Heatmap of steering angle classes vs RoadOptions
      - 2D trajectory
      - Velocity over time
      - CrossTrackError over time
      - Mode-segmented CTE
      - Distribution of max confidence (if available)
      - Additional plot: correlation between confidence and future CTE
    """

    # (A) RoadOptionDistribution
    if 'CurrentRoadOption' in data.columns:
        road_option_counts = data['CurrentRoadOption'].value_counts()
        plt.figure(figsize=(8, 5))
        road_option_counts.plot(kind='bar', rot=45, color='skyblue', edgecolor='black')
        plt.title('Occurrences of Road Options')
        plt.xlabel('Road Option')
        plt.ylabel('Count')
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'road_option_distribution.png'))
        plt.close()

    # (B) Steering Angle Class Distribution (PilotNet classes)
    if 'PilotNetSteeringClass' in data.columns:
        class_counts = data['PilotNetSteeringClass'].value_counts(sort=False)
        # Sort by class index
        class_counts = class_counts.reindex(range(len(STEERING_BOUNDARIES)), fill_value=0)
        labels = [steering_class_to_label(i, STEERING_BOUNDARIES) for i in class_counts.index]

        plt.figure(figsize=(10, 5))
        plt.bar(labels, class_counts.values, color='green', edgecolor='black')
        plt.title('Distribution of PilotNet Steering Angle Classes')
        plt.xlabel('Steering Class (Nearest Boundary)')
        plt.ylabel('Frequency')
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'pilotnet_steering_class_distribution.png'))
        plt.close()

    # (C) Heatmap of Steering Classes vs Road Options
    if 'PilotNetSteeringClass' in data.columns and 'CurrentRoadOption' in data.columns:
        crosstab = pd.crosstab(data['PilotNetSteeringClass'], data['CurrentRoadOption'])
        # Sort rows by class index
        crosstab = crosstab.reindex(range(len(STEERING_BOUNDARIES)), axis=0, fill_value=0)
        row_labels = [steering_class_to_label(i) for i in crosstab.index]
        # Heatmap
        plt.figure(figsize=(10, 6))
        sns.heatmap(crosstab, annot=True, fmt='d', cmap='YlGnBu',
                    xticklabels=crosstab.columns, yticklabels=row_labels)
        plt.title("Heatmap: Steering Angle Classes vs Road Options")
        plt.xlabel("RoadOption")
        plt.ylabel("PilotNetSteeringClass (Nearest Boundary)")
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'steering_class_vs_road_option_heatmap.png'))
        plt.close()

    # (D) 2D Trajectory
    if 'X' in data.columns and 'Y' in data.columns:
        plt.figure(figsize=(10, 6))
        plt.plot(data['Y'], data['X'], color='red')
        plt.title('2D Diagram of Vehicle X/Y Positions')
        plt.xlabel('Y')
        plt.ylabel('X')
        plt.grid(True)
        plt.savefig(os.path.join(output_dir, 'xy_trajectory.png'))
        plt.close()

    # (E) Velocity Over Time
    if 'Velocity' in data.columns:
        plt.figure(figsize=(12, 6))
        plt.plot(data['Timestamp'], data['Velocity'], color='blue')
        plt.title('Vehicle Velocity Over Time')
        plt.xlabel('Timestamp (s)')
        plt.ylabel('Velocity (m/s)')
        plt.grid(True)
        plt.savefig(os.path.join(output_dir, 'velocity_over_time.png'))
        plt.close()

    # (F) CrossTrackError Over Time
    if 'CrossTrackError' in data.columns:
        plt.figure(figsize=(12, 6))
        plt.plot(data['Timestamp'], data['CrossTrackError'], color='purple')
        plt.title('Cross Track Error Over Time')
        plt.xlabel('Timestamp (s)')
        plt.ylabel('Cross Track Error (m)')
        plt.grid(True)
        plt.savefig(os.path.join(output_dir, 'cte_over_time.png'))
        plt.close()

    # (G) Mode-segmented CTE vs Distance
    if 'ExecutedBy' in data.columns and 'Meters' in data.columns and 'CrossTrackError' in data.columns:
        data['SegmentID'] = (data['ExecutedBy'] != data['ExecutedBy'].shift()).cumsum()
        color_map = {
            'PilotNet': 'blue',
            'Stanley': 'red',
            'Intervention(Manual)': 'orange',
            'Intervention(LowSpeed)': 'magenta'
        }

        plt.figure(figsize=(12, 6))
        prev_end = None
        for seg_id, seg_data in data.groupby('SegmentID'):
            mode = seg_data['ExecutedBy'].iloc[0]
            c = color_map.get(mode, 'gray')
            plt.plot(seg_data['Meters'], seg_data['CrossTrackError'], color=c, alpha=0.8)
            if prev_end is not None:
                plt.plot([prev_end['Meters'], seg_data.iloc[0]['Meters']],
                         [prev_end['CrossTrackError'], seg_data.iloc[0]['CrossTrackError']],
                         color='gray', linestyle='--', alpha=0.4)
            prev_end = seg_data.iloc[-1]

        legend_elems = [
            Line2D([0], [0], color='blue', lw=2, label='PilotNet'),
            Line2D([0], [0], color='red', lw=2, label='Stanley'),
            Line2D([0], [0], color='orange', lw=2, label='Intervention(Manual)'),
            Line2D([0], [0], color='magenta', lw=2, label='Intervention(LowSpeed)'),
        ]
        plt.legend(handles=legend_elems, loc='upper right')
        plt.title('CrossTrackError vs Distance (Colored by Steering Mode)')
        plt.xlabel('Distance (m)')
        plt.ylabel('CrossTrackError (m)')
        plt.grid(True)
        plt.savefig(os.path.join(output_dir, 'cte_mode_segments.png'))
        plt.close()

    # (H) Distribution of max confidence (if available)
    prob_cols = [col for col in data.columns if col.startswith("Probability_Class_")]
    if prob_cols and 'MaxPredictionConfidence' in data.columns:
        plt.figure(figsize=(10, 5))
        plt.hist(data['MaxPredictionConfidence'].dropna(), bins=20, color='green', edgecolor='black')
        plt.title('Distribution of Max PilotNet Prediction Confidence')
        plt.xlabel('Confidence')
        plt.ylabel('Frequency')
        plt.grid(True)
        plt.savefig(os.path.join(output_dir, 'pilotnet_confidence_distribution.png'))
        plt.close()

    # (I) Plot time-lag correlation between confidence and future CTE
    lag_dict = compute_time_lag_confidence_cte(data, max_lag_seconds=5.0, step=0.1)
    if lag_dict:
        # Make a simple line plot of shift vs correlation
        plt.figure(figsize=(8, 5))
        shifts = list(lag_dict.keys())
        corr_vals = list(lag_dict.values())
        # Sort by numeric shift
        numeric_shifts = [float(s.replace('s','')) for s in shifts]
        # Pair them up and sort
        combined = sorted(zip(numeric_shifts, corr_vals), key=lambda x: x[0])
        sorted_shifts = [f"{c[0]:.1f}" for c in combined]
        sorted_corrs = [c[1] for c in combined]
        plt.plot(np.asarray(sorted_shifts, float), np.asarray(sorted_corrs), marker='o', color='darkblue')
        plt.title('Correlation of Confidence(t) with Abs CTE(t+lag)')
        plt.xlabel('Time Lag (s)')
        plt.ylabel('Pearson Correlation')
        plt.grid(True)
        plt.savefig(os.path.join(output_dir, 'confidence_future_cte_correlation.png'))
        plt.close()

    # (J) Entropy Over Time
    plt.figure(figsize=(12, 6))
    plt.plot(data['Timestamp'], data['Entropy'], label='Entropy', color='orange')
    plt.scatter(data[data['IsCarAtJunction']]['Timestamp'],
                data[data['IsCarAtJunction']]['Entropy'],
                color='red', label='Junctions', zorder=5)
    plt.title('Entropy Over Time')
    plt.xlabel('Timestamp (s)')
    plt.ylabel('Entropy')
    plt.legend()
    plt.grid(True)
    plt.savefig(os.path.join(output_dir, 'entropy_over_time.png'))
    plt.close()

    # (K) Entropy at Junctions
    uncertainty_by_road_option = data.groupby('CurrentRoadOption')['Entropy'].mean()
    plt.figure(figsize=(8, 6))
    sns.heatmap(uncertainty_by_road_option.to_frame().T, annot=True, cmap='YlGnBu')
    plt.title('Average Entropy by Road Option')
    plt.savefig(os.path.join(output_dir, 'uncertainty_heatmap.png'))
    plt.close()


    # (L) Entropy Density at Junctions
    junction_entropy = data.loc[data['IsCarAtJunction'], 'Entropy'].dropna()
    plt.figure(figsize=(10, 5))
    # Histogram
    plt.subplot(1, 2, 1)
    sns.histplot(junction_entropy, bins=20, kde=False, color='orange', edgecolor='black')
    plt.title('Histogram of Entropy at Junctions')
    plt.xlabel('Entropy')
    plt.ylabel('Frequency')
    plt.grid(True)

    # KDE Plot
    plt.subplot(1, 2, 2)
    sns.kdeplot(junction_entropy, color='blue', linewidth=2)
    plt.title('KDE of Entropy at Junctions')
    plt.xlabel('Entropy')
    plt.ylabel('Density')
    plt.grid(True)

    plt.savefig(os.path.join(output_dir, 'junction_entropy_hist.png'))
    plt.close()

    # (J) Entropy Density for Hight CTE
    cte_entropy = data.loc[data['CrossTrackError'].abs() > HIGH_CTE_THRESH, 'Entropy'].dropna()
    plt.figure(figsize=(10, 5))
    # Histogram
    plt.subplot(1, 2, 1)
    sns.histplot(cte_entropy, bins=20, kde=False, color='orange', edgecolor='black')
    plt.title(f'Histogram of Entropy at with High CTE = {HIGH_CTE_THRESH}')
    plt.xlabel('Entropy')
    plt.ylabel('Frequency')
    plt.grid(True)

    # KDE Plot
    plt.subplot(1, 2, 2)
    sns.kdeplot(cte_entropy, color='blue', linewidth=2)
    plt.title(f'KDE of Entropy at with High CTE = {HIGH_CTE_THRESH}')
    plt.xlabel('Entropy')
    plt.ylabel('Density')
    plt.grid(True)

    plt.savefig(os.path.join(output_dir, 'cte_entropy_hist.png'))
    plt.close()

    # (K) Entropy Density Overall
    entropy = data['Entropy'].dropna()
    plt.figure(figsize=(10, 5))
    # Histogram
    plt.subplot(1, 2, 1)
    sns.histplot(entropy, bins=20, kde=False, color='orange', edgecolor='black')
    plt.title(f'Histogram of Entropy Overall')
    plt.xlabel('Entropy')
    plt.ylabel('Frequency')
    plt.grid(True)

    # KDE Plot
    plt.subplot(1, 2, 2)
    sns.kdeplot(entropy, color='blue', linewidth=2)
    plt.title('KDE of Entropy Overall')
    plt.xlabel('Entropy')
    plt.ylabel('Density')
    plt.grid(True)

    plt.savefig(os.path.join(output_dir, 'overall_entropy_hist.png'))
    plt.close()

    # (J) Entropy Density for Hight CTE
    plt.figure(figsize=(10, 5))
    sns.kdeplot(cte_entropy, color='orange', linewidth=2, label='High CTE Entropy')
    sns.kdeplot(entropy, color='blue', linewidth=2, label='Overall Entropy')
    sns.kdeplot(junction_entropy, color='red', linewidth=2, label='Junction Entropy')
    plt.title(f'KDE of Entropy')
    plt.xlabel('Entropy')
    plt.ylabel('Density')
    plt.legend()
    plt.grid(True)

    plt.savefig(os.path.join(output_dir, 'entropy_hist.png'))
    plt.close()



def format_value(v):
    """Helper function to format values with 4 decimal places"""
    if isinstance(v, (float, np.float32, np.float64)):
        return f"{v:.4f}"
    return str(v)

# Reporting
def write_report(general_metrics, intervention_metrics, failure_analysis, entropy_analysis, classification_metrics, output_dir):
    """
    Writes a simple markdown report to output_dir (report.md),
    with references to generated plots and the computed metrics.
    """
    report_path = os.path.join(output_dir, 'report.md')
    with open(report_path, 'w') as f:
        f.write("# Analysis Report\n\n")

        # General metrics with detailed CTE analysis
        f.write("## General Metrics\n")
        f.write("### Basic Metrics\n")
        for k, v in general_metrics.items():
            if k.startswith("CTE") or k == "Overall Abs Mean CTE (m)":
                continue  # We'll show these separately
            if isinstance(v, dict):
                f.write(f"- **{k}**:\n")
                for sub_k, sub_v in v.items():
                    f.write(f"  - {sub_k}: {format_value(sub_v)}\n")
            else:
                f.write(f"- **{k}**: {format_value(v)}\n")

        # Detailed CTE Analysis
        f.write("\n### Cross Track Error Analysis\n")
        f.write(f"- **Overall Abs Mean CTE (m)**: {format_value(general_metrics['Overall Abs Mean CTE (m)'])}\n")
        f.write(f"- **LaneFollow Abs Mean CTE (m)**: {format_value(general_metrics['LaneFollow Abs Mean CTE (m)'])}\n")
        f.write(f"- **Junction Abs Mean CTE (m)**: {format_value(general_metrics['Junction Abs Mean CTE (m)'])}\n")

        # Intervention metrics with junction-specific counts
        f.write("\n## Intervention Metrics\n")
        f.write(f"- **Total Interventions**: {intervention_metrics['InterventionsCount']}\n")
        f.write(f"- **Junction Interventions**: {intervention_metrics['JunctionInterventionsCount']}\n")
        for k, v in intervention_metrics.items():
            if k not in ['InterventionsCount', 'JunctionInterventionsCount']:
                f.write(f"- **{k}**: {format_value(v)}\n")

        # Failures & Confidence
        f.write("## Failure and Confidence Analysis\n")
        for k, v in failure_analysis.items():
            if isinstance(v, dict):
                f.write(f"- **{k}**:\n")
                for sub_k, sub_v in v.items():
                    f.write(f"  - {sub_k}: {sub_v}\n")
            else:
                f.write(f"- **{k}**: {v}\n")
        f.write("\n")

        # Entropy Analysis
        f.write("## Entropy Analysis\n")
        f.write("- **Overall Entropy Stats**:\n")
        for k, v in entropy_analysis['OverallEntropyStats'].items():
            f.write(f"  - {k}: {v}\n")
        f.write("- **Entropy at Junctions**:\n")
        for k, v in entropy_analysis['JunctionEntropy'].items():
            f.write(f"  - {k}: {v}\n")
        f.write("- **Entropy for High CTE**:\n")
        for k, v in entropy_analysis['HighCTEEntropy'].items():
            f.write(f"  - {k}: {v}\n")
        f.write("\n")


        f.write("\n## Steering Classification Metrics\n\n")

        # Overall metrics
        f.write("### Overall Metrics\n\n")
        f.write(f"- Exact Accuracy: {classification_metrics['exact_accuracy']:.3f}\n")
        f.write(f"- Accuracy (±1 class): {classification_metrics['tolerance_1_accuracy']:.3f}\n")
        f.write(f"- Accuracy (±2 classes): {classification_metrics['tolerance_2_accuracy']:.3f}\n")
        f.write(f"- Mean Absolute Error (in classes): {classification_metrics['mean_absolute_error_classes']:.3f}\n\n")

        # Per-class metrics
        f.write("### Per-Class Metrics\n\n")
        f.write("| Steering Angle | Precision | Recall | F1 | Support |\n")
        f.write("|----------------|-----------|--------|----|---------|\n")

        class_labels = classification_metrics['per_class_metrics']['class_labels']
        precision = classification_metrics['per_class_metrics']['precision']
        recall = classification_metrics['per_class_metrics']['recall']
        f1 = classification_metrics['per_class_metrics']['f1']
        support = classification_metrics['per_class_metrics']['support']

        for i in range(len(class_labels)):
            if support[i] > 0:  # Only show classes that appear in the data
                f.write(f"| {class_labels[i]} | {precision[i]:.3f} | {recall[i]:.3f} | "
                        f"{f1[i]:.3f} | {support[i]} |\n")

        # Accuracy by road option
        if classification_metrics['accuracy_by_road_option']:
            f.write("\n### Accuracy by Road Option\n\n")
            f.write("| Road Option | Exact Accuracy | Ordinal Accuracy (±1 class) |\n")
            f.write("|-------------|----------------|-------------------------|\n")
            for road_option in classification_metrics['accuracy_by_road_option'].keys():
                exact_acc = classification_metrics['accuracy_by_road_option'][road_option]
                ordinal_acc = classification_metrics['tolerance_1_accuracy_by_road_option'][road_option]
                f.write(f"| {road_option} | {exact_acc:.3f} | {ordinal_acc:.3f} |\n")


        f.write("Report generated by analyze_logs.py\n")


def write_excel_friendly_metrics(data, output_dir):
    """
    Writes key metrics to a tab-separated file for easy Excel import.
    Now includes junction count and junction intervention metrics.
    """
    metrics_path = os.path.join(output_dir, 'key_metrics.tsv')

    # Calculate all needed metrics
    total_distance = round(data['Meters'].iloc[-1] / 1000.0, 4)

    # CTE Analysis
    overall_cte = round(data['CrossTrackError'].abs().mean(), 4)
    lanefollow_cte = round(data.loc[data['CurrentRoadOption'] == 'LANEFOLLOW', 'CrossTrackError'].abs().mean(), 4)
    junction_cte = round(data.loc[data['IsCarAtJunction'] == True, 'CrossTrackError'].abs().mean(), 4)

    # Count total junctions (transitions into junction areas)
    data['PrevIsJunction'] = data['IsCarAtJunction'].shift(1).fillna(False)
    total_junctions = len(data[(data['IsCarAtJunction'] == True) & (data['PrevIsJunction'] == False)])

    # Intervention Analysis
    data['PrevExecutedBy'] = data['ExecutedBy'].shift(1)
    transitions = data[
        (data['PrevExecutedBy'] == 'PilotNet') &
        (data['ExecutedBy'].isin(['Stanley', 'Intervention(Manual)', 'Intervention(LowSpeed)']))
        ]
    total_interventions = len(transitions)
    junction_interventions = len(transitions[transitions['IsCarAtJunction'] == True])

    juntion_sucess_percentage = round(100 - ((junction_interventions / total_junctions) * 100), 4) if total_junctions > 0 else 0.0

    # Calculate percentage of time in PilotNet control
    total_time = data['time_delta'].sum()
    pilotnet_time = data[data['ExecutedBy'] == 'PilotNet']['time_delta'].sum()
    pilotnet_percentage = round((pilotnet_time / total_time) * 100, 4) if total_time > 0 else 0.0

    # Compute exactness metrics for steering
    y_true = data['StanleySteeringAngle'].apply(lambda x: pilotnet.map_label_to_class(x))
    y_pred = data['PilotNetSteeringAngle'].apply(lambda x: pilotnet.map_label_to_class(x))

    exact_accuracy = round(accuracy_score(y_true, y_pred), 4)
    ordinal_1_accuracy = round(compute_ordinal_accuracy(y_true, y_pred, tolerance=1), 4)
    ordinal_2_accuracy = round(compute_ordinal_accuracy(y_true, y_pred, tolerance=2), 4)

    # Write headers and values
    with open(metrics_path, 'w') as f:
        # Write headers
        headers = [
            'Total_Distance_km',
            'Total_Junctions',
            'Overall_Abs_CTE',
            'LaneFollow_Abs_CTE',
            'Junction_Abs_CTE',
            'Total_Interventions',
            'Junction_Interventions',
            'Junction_Sucess_Percentage',
            'PilotNet_Time_Percentage',
            'Exact_Accuracy',
            'Accuracy_Within_1',
            'Accuracy_Within_2'
        ]
        f.write('\t'.join(headers) + '\n')

        # Write values
        values = [
            f"{total_distance}",
            f"{total_junctions}",
            f"{overall_cte}",
            f"{lanefollow_cte}",
            f"{junction_cte}",
            f"{total_interventions}",
            f"{junction_interventions}",
            f"{juntion_sucess_percentage}",
            f"{pilotnet_percentage}",
            f"{exact_accuracy}",
            f"{ordinal_1_accuracy}",
            f"{ordinal_2_accuracy}"
        ]
        f.write('\t'.join(values) + '\n')

# 6) Main
def analyze(csv_path):
    output_dir = os.path.dirname(os.path.abspath(csv_path))
    if not os.path.isdir(output_dir):
        print(f"Error: Output directory not found: {output_dir}")
        sys.exit(1)

    # 1) Load & preprocess
    data = load_and_preprocess_data(csv_path)

    # 2) Compute metrics
    general_metrics = compute_general_metrics(data)
    intervention_metrics = compute_intervention_metrics(data)
    failure_analysis = analyze_failures_and_confidence(data)
    entropy_analysis = analyze_entropy(data)
    classification_metrics = compute_steering_classification_metrics(data)

    # 3) Generate plots
    generate_plots(data, output_dir)
    plot_classification_metrics(classification_metrics, output_dir)

    # 4) Write markdown report
    write_report(general_metrics, intervention_metrics, failure_analysis, entropy_analysis, classification_metrics, output_dir)

    # 5) Print summary to console
    print("=== General Metrics ===")
    for k, v in general_metrics.items():
        print(f"{k}: {v}")

    print("\n=== Intervention Metrics ===")
    for k, v in intervention_metrics.items():
        print(f"{k}: {v}")

    print("\n=== Failure & Confidence Analysis ===")
    for k, v in failure_analysis.items():
        print(f"{k}: {v}")

    print("\n=== Entropy Analysis ===")
    print("- Overall Entropy Stats:")
    for k, v in entropy_analysis['OverallEntropyStats'].items():
        print(f"  - {k}: {v}")
    print("- Junction Entropy:")
    for k, v in entropy_analysis['JunctionEntropy'].items():
        print(f"  - {k}: {v}")
    print("- High CTE Entropy:")
    for k, v in entropy_analysis['HighCTEEntropy'].items():
        print(f"  - {k}: {v}")

    print("\n=== Steering Classification Metrics ===")
    # print confusion matrix
    print(classification_metrics['confusion_matrix'])
    print(f"Exact Accuracy: {classification_metrics['exact_accuracy']:.3f}")
    print(f"Ordinal Accuracy (±1 class): {classification_metrics['tolerance_1_accuracy']:.3f}")
    print(f"Ordinal Accuracy (±2 classes): {classification_metrics['tolerance_2_accuracy']:.3f}")
    print(f"Mean Absolute Error (in classes): {classification_metrics['mean_absolute_error_classes']:.3f}")
    print(f"Accuracy by Road Option: {classification_metrics['accuracy_by_road_option']}")
    print(f"Accuracy by Road Option (±1 class): {classification_metrics['tolerance_1_accuracy_by_road_option']}")

    print(f"\nDone. Analysis saved to {output_dir} (report.md + plots).")

    # Print InterventionsCount, Abs Mean CTE (m), Exact Accuracy, Accuracy (±1 class), Accuracy (±2 classes) all seperated by tab
    print(f"{intervention_metrics['InterventionsCount']}\t{general_metrics['Overall Abs Mean CTE (m)']:.3f}\t{classification_metrics['exact_accuracy']:.3f}\t{classification_metrics['tolerance_1_accuracy']:.3f}\t{classification_metrics['tolerance_2_accuracy']:.3f}")

    write_excel_friendly_metrics(data, output_dir)

    return os.path.join(output_dir, 'report.md')




def main():
    """
    Main entry point:
      python analyze_logs.py --csv /path/to/session_data.csv

    This script will:
      - Load & preprocess the CSV
      - Compute metrics
      - Generate plots (heatmaps, distributions, time-lag correlation, etc.)
      - Write a summary markdown report with references to the plots
    """
    import sys
    import argparse

    parser = argparse.ArgumentParser(description="Analyze PilotNet session logs with additional creative metrics.")
    parser.add_argument("--csv", type=str, required=True, help="Path to session_data.csv file.")
    args = parser.parse_args()

    csv_path = args.csv
    if not os.path.exists(csv_path):
        print(f"Error: CSV file not found: {csv_path}")
        sys.exit(1)

    analyze(csv_path)



if __name__ == "__main__":
    main()
    #example usage: python analyze_logs.py --csv /path/to/session_data.csv